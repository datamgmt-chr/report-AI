<!DOCTYPE html>
<html lang="it">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Scegliere e Configurare il Modello LLM</title>
<link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,600;0,700;0,900;1,400;1,600&family=JetBrains+Mono:wght@300;400;500;700&family=Outfit:wght@300;400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --bg:    #0e0d0b;
  --bg2:   #161410;
  --bg3:   #1e1c17;
  --bg4:   #262420;
  --ink:   #f0ece0;
  --ink2:  #c8c4b0;
  --ink3:  #8a8678;
  --ink4:  #504e44;
  --line:  #2a2820;
  --line2: #3a3830;

  --gold:  #c8962a;
  --goldl: #e8b84a;
  --goldd: #8a6010;
  --amber: #d4760a;
  --teal:  #2a8a7a;
  --teal2: #1a5a4e;
  --blue:  #2a5aaa;
  --red:   #aa3a2a;
  --green: #3a8a3a;
}

* { margin:0; padding:0; box-sizing:border-box; }
html { scroll-behavior:smooth; }

body {
  background: var(--bg);
  color: var(--ink);
  font-family: 'Outfit', sans-serif;
  font-weight: 300;
  font-size: 14px;
  line-height: 1.7;
}

/* ── MASTHEAD ── */
.masthead {
  padding: 64px 60px 56px;
  border-bottom: 1px solid var(--line2);
  position: relative;
  overflow: hidden;
}

.masthead::before {
  content: '';
  position: absolute;
  top: 0; right: 0;
  width: 40%;
  height: 100%;
  background: radial-gradient(ellipse at top right, rgba(200,150,42,0.06) 0%, transparent 65%);
  pointer-events: none;
}

.mh-overline {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: 3px;
  text-transform: uppercase;
  color: var(--gold);
  margin-bottom: 20px;
  display: flex;
  align-items: center;
  gap: 12px;
}

.mh-overline::before {
  content: '';
  display: block;
  width: 32px;
  height: 1px;
  background: var(--gold);
}

.masthead h1 {
  font-family: 'Playfair Display', serif;
  font-size: clamp(36px, 5.5vw, 72px);
  font-weight: 900;
  line-height: 0.95;
  letter-spacing: -1.5px;
  margin-bottom: 6px;
}

.masthead h1 em {
  font-style: italic;
  font-weight: 400;
  color: var(--ink2);
}

.masthead-sub {
  font-size: 15px;
  color: var(--ink3);
  max-width: 580px;
  margin-top: 20px;
  line-height: 1.75;
}

.masthead-sub strong { color: var(--ink2); font-weight: 500; }

.mh-chapters {
  display: flex;
  gap: 6px;
  margin-top: 32px;
  flex-wrap: wrap;
}

.mh-ch {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  padding: 5px 14px;
  border: 1px solid var(--line2);
  color: var(--ink3);
  letter-spacing: 1px;
  cursor: pointer;
  transition: all 0.15s;
}

.mh-ch:hover { border-color: var(--gold); color: var(--gold); }

/* ── NAV ── */
.chapter-nav {
  position: sticky;
  top: 0;
  z-index: 100;
  background: var(--bg2);
  border-bottom: 1px solid var(--line2);
  display: flex;
  overflow-x: auto;
}

.cn-tab {
  flex-shrink: 0;
  padding: 14px 24px 12px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--ink4);
  border: none;
  background: transparent;
  cursor: pointer;
  border-right: 1px solid var(--line);
  transition: color 0.15s;
  border-bottom: 2px solid transparent;
}

.cn-tab:hover { color: var(--ink2); }
.cn-tab.active { color: var(--gold); border-bottom-color: var(--gold); }

/* ── PANELS ── */
.chapter { display: none; }
.chapter.active { display: block; animation: fadeIn 0.2s ease; }
@keyframes fadeIn { from { opacity:0; transform:translateY(4px); } to { opacity:1; transform:none; } }

/* ── SECTION ── */
.section {
  padding: 52px 60px;
  border-bottom: 1px solid var(--line);
}

.section:last-child { border-bottom: none; }

.sec-label {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: 3px;
  text-transform: uppercase;
  color: var(--gold);
  margin-bottom: 10px;
  display: flex;
  align-items: center;
  gap: 10px;
}

.sec-label::before { content: '§'; }

.sec-h2 {
  font-family: 'Playfair Display', serif;
  font-size: clamp(24px, 3.5vw, 44px);
  font-weight: 700;
  line-height: 1.05;
  letter-spacing: -0.5px;
  margin-bottom: 14px;
}

.sec-intro {
  font-size: 14px;
  color: var(--ink2);
  max-width: 660px;
  line-height: 1.8;
  margin-bottom: 36px;
}

.sec-intro strong { color: var(--ink); font-weight: 500; }
.sec-intro em { font-style: italic; color: var(--ink3); }

/* ── DECISION FRAMEWORK ── */
.decision-grid {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 1px;
  background: var(--line2);
  border: 1px solid var(--line2);
  margin-bottom: 32px;
}

.dec-card {
  background: var(--bg2);
  padding: 24px 20px;
}

.dc-num {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--ink4);
  letter-spacing: 2px;
  margin-bottom: 10px;
}

.dc-question {
  font-family: 'Playfair Display', serif;
  font-size: 17px;
  font-weight: 600;
  line-height: 1.25;
  margin-bottom: 14px;
  color: var(--ink);
}

.dc-answers {
  list-style: none;
  display: flex;
  flex-direction: column;
  gap: 6px;
}

.dc-answers li {
  font-size: 12px;
  color: var(--ink3);
  padding: 7px 12px;
  background: var(--bg3);
  border-left: 2px solid var(--line2);
  line-height: 1.4;
  display: flex;
  gap: 8px;
}

.dc-answers li .ans-label {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  color: var(--gold);
  flex-shrink: 0;
  padding-top: 1px;
  letter-spacing: 1px;
}

.dc-answers li .ans-text { color: var(--ink2); }

/* ── MODEL CARDS ── */
.model-grid {
  display: grid;
  grid-template-columns: repeat(2, 1fr);
  gap: 1px;
  background: var(--line2);
  border: 1px solid var(--line2);
  margin-bottom: 32px;
}

.model-card {
  background: var(--bg2);
  padding: 28px 24px;
  position: relative;
  overflow: hidden;
}

.model-card::after {
  content: '';
  position: absolute;
  top: 0; left: 0;
  width: 3px;
  height: 100%;
}

.mc-tier {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  letter-spacing: 2px;
  text-transform: uppercase;
  margin-bottom: 10px;
}

.mc-name {
  font-family: 'Playfair Display', serif;
  font-size: 22px;
  font-weight: 700;
  margin-bottom: 6px;
  line-height: 1.1;
}

.mc-family {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--ink4);
  margin-bottom: 16px;
  letter-spacing: 1px;
}

.mc-desc {
  font-size: 12px;
  color: var(--ink3);
  line-height: 1.6;
  margin-bottom: 16px;
}

.mc-desc strong { color: var(--ink2); font-weight: 500; }

.mc-tags {
  display: flex;
  flex-wrap: wrap;
  gap: 5px;
  margin-bottom: 14px;
}

.mc-tag {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  padding: 3px 8px;
  border: 1px solid var(--line2);
  color: var(--ink4);
  letter-spacing: 1px;
}

.mc-tag.best { border-color: var(--teal); color: #4acab8; }
.mc-tag.good { border-color: var(--goldd); color: var(--goldl); }
.mc-tag.avoid { border-color: var(--red); color: #e06050; }

.mc-stats {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 1px;
  background: var(--line);
  border: 1px solid var(--line);
  margin-top: 14px;
}

.mc-stat {
  background: var(--bg3);
  padding: 8px;
  text-align: center;
}

.mc-stat-n {
  font-family: 'JetBrains Mono', monospace;
  font-size: 14px;
  font-weight: 700;
  display: block;
  margin-bottom: 2px;
}

.mc-stat-l {
  font-family: 'JetBrains Mono', monospace;
  font-size: 8px;
  letter-spacing: 1px;
  text-transform: uppercase;
  color: var(--ink4);
}

/* Tier colors */
.tier-frontier .mc-tier { color: #c0a050; }
.tier-frontier::after { background: #c0a050; }
.tier-frontier .mc-name { color: #e0c070; }

.tier-mid .mc-tier { color: var(--teal); }
.tier-mid::after { background: var(--teal); }
.tier-mid .mc-name { color: #5adac8; }

.tier-fast .mc-tier { color: #7a8aba; }
.tier-fast::after { background: #7a8aba; }
.tier-fast .mc-name { color: #9aaae0; }

.tier-local .mc-tier { color: var(--ink3); }
.tier-local::after { background: var(--ink3); }

/* ── TASK MATRIX ── */
.task-matrix {
  width: 100%;
  border-collapse: collapse;
  border: 1px solid var(--line2);
  margin-bottom: 32px;
  font-size: 12px;
}

.task-matrix th {
  padding: 12px 14px;
  background: var(--bg3);
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--ink4);
  border-right: 1px solid var(--line);
  border-bottom: 1px solid var(--line2);
  text-align: left;
  font-weight: 400;
}

.task-matrix th:last-child { border-right: none; }

.task-matrix td {
  padding: 11px 14px;
  border-bottom: 1px solid var(--line);
  border-right: 1px solid var(--line);
  color: var(--ink3);
  vertical-align: top;
  line-height: 1.5;
}

.task-matrix td:last-child { border-right: none; }
.task-matrix tr:last-child td { border-bottom: none; }
.task-matrix tr:hover td { background: var(--bg3); }

.task-matrix .task-name { color: var(--ink); font-weight: 500; font-size: 13px; }

.cell-best  { color: #4acab8 !important; font-weight: 500; }
.cell-good  { color: var(--goldl) !important; }
.cell-avoid { color: #e06050 !important; }
.cell-note  { color: var(--ink4) !important; font-style: italic; font-size: 11px; }

/* ── PARAMETER CONFIG ── */
.param-table {
  border: 1px solid var(--line2);
  overflow: hidden;
  margin-bottom: 32px;
}

.pt-header {
  background: var(--bg3);
  padding: 12px 20px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--ink4);
  border-bottom: 1px solid var(--line2);
  display: flex;
  justify-content: space-between;
}

.pt-row {
  display: grid;
  grid-template-columns: 160px 130px 1fr 200px;
  border-bottom: 1px solid var(--line);
  background: var(--bg2);
}

.pt-row:last-child { border-bottom: none; }
.pt-row:hover { background: var(--bg3); }
.pt-row.head { background: var(--bg3); }

.pt-row.head .ptc {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--ink4);
  padding: 10px 14px;
  border-right: 1px solid var(--line);
  font-weight: 400;
}

.ptc {
  padding: 13px 14px;
  border-right: 1px solid var(--line);
  font-size: 12px;
  color: var(--ink3);
  line-height: 1.5;
  vertical-align: top;
}

.ptc:last-child { border-right: none; }

.ptc .param-name {
  font-family: 'JetBrains Mono', monospace;
  font-size: 12px;
  color: var(--goldl);
  display: block;
  margin-bottom: 2px;
}

.ptc .param-range {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--ink4);
}

.ptc strong { color: var(--ink2); font-weight: 500; }

.val-chip {
  display: inline-block;
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  padding: 2px 8px;
  margin: 2px 2px 2px 0;
  border: 1px solid var(--line2);
  color: var(--ink3);
}

.val-chip.rec { border-color: var(--teal2); color: #4acab8; }

/* ── CODE BLOCKS ── */
.code-wrap {
  background: var(--bg);
  border: 1px solid var(--line2);
  border-left: 3px solid var(--gold);
  overflow: hidden;
  margin-bottom: 24px;
}

.code-label {
  background: var(--bg3);
  padding: 8px 16px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: 2px;
  color: var(--ink4);
  border-bottom: 1px solid var(--line2);
  text-transform: uppercase;
}

.code-body {
  padding: 16px 20px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 12px;
  line-height: 1.8;
  overflow-x: auto;
}

.cm { color: var(--ink4); }
.kw { color: #7ab0f0; }
.fn { color: #f0c050; }
.st { color: #88d060; }
.nu { color: #f09050; }
.pl { color: var(--ink2); }
.hl { color: var(--goldl); }

/* ── EVAL FRAMEWORK ── */
.eval-phases {
  display: grid;
  grid-template-columns: 48px 1fr;
  border: 1px solid var(--line2);
  overflow: hidden;
  margin-bottom: 32px;
}

.ep-num {
  background: var(--bg3);
  border-right: 1px solid var(--line2);
  display: flex;
  align-items: flex-start;
  justify-content: center;
  padding-top: 18px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 12px;
  color: var(--gold);
  font-weight: 700;
}

.ep-content {
  padding: 18px 20px;
  border-bottom: 1px solid var(--line);
  background: var(--bg2);
}

.ep-content:last-child { border-bottom: none; }
.ep-content:hover { background: var(--bg3); }

.ep-title {
  font-size: 14px;
  font-weight: 600;
  margin-bottom: 6px;
  color: var(--ink);
}

.ep-desc {
  font-size: 12px;
  color: var(--ink3);
  line-height: 1.6;
}

.ep-desc strong { color: var(--ink2); font-weight: 500; }

/* ── BENCHMARK TABLE ── */
.bench-table {
  width: 100%;
  border-collapse: collapse;
  border: 1px solid var(--line2);
  font-size: 12px;
  margin-bottom: 32px;
}

.bench-table th {
  background: var(--bg3);
  padding: 11px 14px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  letter-spacing: 1.5px;
  text-transform: uppercase;
  color: var(--ink4);
  border-right: 1px solid var(--line);
  border-bottom: 1px solid var(--line2);
  text-align: left;
  font-weight: 400;
}

.bench-table th:last-child { border-right: none; }

.bench-table td {
  padding: 10px 14px;
  border-bottom: 1px solid var(--line);
  border-right: 1px solid var(--line);
  color: var(--ink3);
  vertical-align: top;
  line-height: 1.5;
}

.bench-table td:last-child { border-right: none; }
.bench-table tr:last-child td { border-bottom: none; }
.bench-table tr:hover td { background: var(--bg3); }

.bench-table .bname { color: var(--ink); font-weight: 500; font-size: 13px; }
.bench-table .bwarn { color: #e06050; font-size: 11px; font-style: italic; }

/* ── COST VISUAL ── */
.cost-tiers {
  display: grid;
  grid-template-columns: repeat(4, 1fr);
  gap: 1px;
  background: var(--line2);
  border: 1px solid var(--line2);
  margin-bottom: 32px;
}

.cost-tier {
  background: var(--bg2);
  padding: 22px 16px;
}

.ct-tier {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--ink4);
  margin-bottom: 10px;
}

.ct-cost {
  font-family: 'Playfair Display', serif;
  font-size: 28px;
  font-weight: 700;
  line-height: 1;
  margin-bottom: 4px;
}

.ct-unit {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--ink4);
  margin-bottom: 14px;
}

.ct-models {
  font-size: 11px;
  color: var(--ink3);
  line-height: 1.6;
}

.ct-models strong { color: var(--ink2); }

/* ── OPTIMIZATION CARDS ── */
.opt-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 1px;
  background: var(--line2);
  border: 1px solid var(--line2);
  margin-bottom: 32px;
}

.opt-card {
  background: var(--bg2);
  padding: 24px 20px;
}

.oc-label {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--gold);
  margin-bottom: 10px;
}

.oc-title {
  font-size: 15px;
  font-weight: 600;
  margin-bottom: 10px;
  color: var(--ink);
  line-height: 1.2;
}

.oc-desc {
  font-size: 12px;
  color: var(--ink3);
  line-height: 1.65;
  margin-bottom: 12px;
}

.oc-desc strong { color: var(--ink2); font-weight: 500; }

.oc-saving {
  font-family: 'JetBrains Mono', monospace;
  font-size: 11px;
  padding: 6px 12px;
  border: 1px solid var(--teal2);
  color: #4acab8;
  display: inline-block;
}

/* ── RULES ── */
.rules-list {
  display: grid;
  gap: 1px;
  background: var(--line2);
  border: 1px solid var(--line2);
  margin-bottom: 32px;
}

.rule-row {
  background: var(--bg2);
  display: grid;
  grid-template-columns: 56px 1fr;
  min-height: 64px;
}

.rule-row:hover { background: var(--bg3); }

.rr-num {
  background: var(--bg3);
  border-right: 1px solid var(--line2);
  display: flex;
  align-items: center;
  justify-content: center;
  font-family: 'Playfair Display', serif;
  font-size: 24px;
  font-weight: 900;
  color: var(--gold);
  opacity: 0.5;
}

.rr-content {
  padding: 16px 20px;
}

.rr-title {
  font-size: 14px;
  font-weight: 600;
  margin-bottom: 5px;
  color: var(--ink);
}

.rr-desc {
  font-size: 12px;
  color: var(--ink3);
  line-height: 1.55;
}

.rr-desc strong { color: var(--ink2); font-weight: 500; }

/* ── ALERT BOX ── */
.alert {
  padding: 18px 20px;
  margin-bottom: 24px;
  display: grid;
  grid-template-columns: 28px 1fr;
  gap: 12px;
  align-items: start;
  border: 1px solid;
}

.alert.warn { background: rgba(200,150,42,0.06); border-color: var(--goldd); }
.alert.info { background: rgba(42,138,122,0.06); border-color: var(--teal2); }
.alert.danger { background: rgba(170,58,42,0.06); border-color: var(--red); }

.alert-icon { font-size: 16px; }

.alert-body { font-size: 12px; color: var(--ink3); line-height: 1.6; }
.alert-body strong { color: var(--ink2); font-weight: 500; }

/* ── PROMPT ANATOMY ── */
.prompt-anatomy {
  border: 1px solid var(--line2);
  overflow: hidden;
  margin-bottom: 32px;
}

.pa-header {
  background: var(--bg3);
  padding: 12px 20px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--ink4);
  border-bottom: 1px solid var(--line2);
}

.pa-segment {
  display: grid;
  grid-template-columns: 180px 1fr;
  border-bottom: 1px solid var(--line);
  background: var(--bg2);
}

.pa-segment:last-child { border-bottom: none; }
.pa-segment:hover { background: var(--bg3); }

.pa-label-cell {
  padding: 14px 16px;
  border-right: 1px solid var(--line2);
  background: var(--bg3);
}

.pa-seg-name {
  font-size: 13px;
  font-weight: 600;
  color: var(--ink);
  margin-bottom: 3px;
}

.pa-seg-opt {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  color: var(--ink4);
  letter-spacing: 1px;
}

.pa-seg-opt.req { color: var(--gold); }

.pa-content-cell {
  padding: 14px 18px;
}

.pa-seg-desc {
  font-size: 12px;
  color: var(--ink3);
  line-height: 1.6;
  margin-bottom: 8px;
}

.pa-seg-desc strong { color: var(--ink2); font-weight: 500; }

.pa-example {
  font-family: 'JetBrains Mono', monospace;
  font-size: 11px;
  color: var(--ink4);
  padding: 8px 12px;
  background: var(--bg);
  border-left: 2px solid var(--gold);
  line-height: 1.6;
}

/* ── FINAL SECTION ── */
.final-section {
  background: var(--bg2);
  padding: 52px 60px;
  border-top: 1px solid var(--line2);
}

/* ── FOOTER ── */
footer {
  background: var(--bg);
  border-top: 1px solid var(--line2);
  padding: 24px 60px;
  display: flex;
  justify-content: space-between;
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--ink4);
  letter-spacing: 1px;
}

/* ── RESPONSIVE ── */
@media (max-width: 900px) {
  .masthead, .section, .final-section, footer { padding-left: 20px; padding-right: 20px; }
  .decision-grid { grid-template-columns: 1fr; }
  .model-grid { grid-template-columns: 1fr; }
  .cost-tiers { grid-template-columns: 1fr 1fr; }
  .opt-grid { grid-template-columns: 1fr; }
  .pt-row { grid-template-columns: 1fr 1fr; }
  .pa-segment { grid-template-columns: 1fr; }
  footer { flex-direction: column; gap: 6px; }
}
</style>
</head>
<body>

<!-- MASTHEAD -->
<div class="masthead">
  <div class="mh-overline">Guida pratica · LLM Selection & Configuration · 2026</div>
  <h1>Scegliere e configurare<br><em>il modello giusto</em></h1>
  <p class="masthead-sub">
    Quattro capitoli operativi: come scegliere il modello per task specifici, come misurarne le performance, come configurare i parametri, come ottimizzare i costi. <strong>Basato su criteri misurabili</strong>, non su preferenze di brand.
  </p>
  <div class="mh-chapters">
    <span class="mh-ch" onclick="showChapter('ch1')">I · Scelta del modello</span>
    <span class="mh-ch" onclick="showChapter('ch2')">II · Valutazione performance</span>
    <span class="mh-ch" onclick="showChapter('ch3')">III · Configurazione</span>
    <span class="mh-ch" onclick="showChapter('ch4')">IV · Costi e ottimizzazione</span>
  </div>
</div>

<!-- NAV -->
<nav class="chapter-nav">
  <button class="cn-tab active" onclick="showChapter('ch1')">I · Scelta del modello</button>
  <button class="cn-tab" onclick="showChapter('ch2')">II · Valutazione performance</button>
  <button class="cn-tab" onclick="showChapter('ch3')">III · Configurazione pratica</button>
  <button class="cn-tab" onclick="showChapter('ch4')">IV · Costi e ottimizzazione</button>
</nav>


<!-- ═══════════════════════════
     CAPITOLO I — SCELTA
═══════════════════════════ -->
<div class="chapter active" id="ch1">

  <!-- 1.1 Framework decisionale -->
  <div class="section">
    <div class="sec-label">Capitolo I · Sezione 1</div>
    <div class="sec-h2">Framework decisionale:<br>come scegliere il modello</div>
    <p class="sec-intro">
      La scelta del modello non si fa guardando i benchmark generali. Si fa rispondendo a <strong>cinque domande specifiche sul tuo task</strong>, in questo ordine. La risposta a ognuna restringe il campo e spesso porta a una scelta ovvia.
    </p>

    <div class="decision-grid">
      <div class="dec-card">
        <div class="dc-num">Domanda 01</div>
        <div class="dc-question">Qual è la natura del ragionamento richiesto?</div>
        <ul class="dc-answers">
          <li><span class="ans-label">SEMPLICE</span><span class="ans-text">Classificazione, estrazione dati, riassunti, template filling → modello veloce ed economico</span></li>
          <li><span class="ans-label">MEDIO</span><span class="ans-text">Risposta a domande su documenti, redazione strutturata, analisi guidata → modello bilanciato</span></li>
          <li><span class="ans-label">COMPLESSO</span><span class="ans-text">Ragionamento multi-step, coding avanzato, analisi ambigua, giudizi sfumati → modello frontier</span></li>
        </ul>
      </div>
      <div class="dec-card">
        <div class="dc-num">Domanda 02</div>
        <div class="dc-question">Quanto è critica la latenza per l'utente?</div>
        <ul class="dc-answers">
          <li><span class="ans-label">REAL-TIME</span><span class="ans-text">Chat interattiva, completamento inline, suggerimenti → latenza &lt;2s obbligatoria → modelli fast</span></li>
          <li><span class="ans-label">TOLLERABILE</span><span class="ans-text">Risposta entro 10–30s accettabile → modelli bilanciati, con streaming</span></li>
          <li><span class="ans-label">BATCH</span><span class="ans-text">Task asincroni, report notturni, elaborazioni → latenza non è vincolo → usa il migliore</span></li>
        </ul>
      </div>
      <div class="dec-card">
        <div class="dc-num">Domanda 03</div>
        <div class="dc-question">I dati possono uscire dall'organizzazione?</div>
        <ul class="dc-answers">
          <li><span class="ans-label">SÌ</span><span class="ans-text">Tutti i modelli cloud disponibili — Claude, GPT, Gemini, Mistral API</span></li>
          <li><span class="ans-label">CON DPA</span><span class="ans-text">Modelli cloud con Data Processing Agreement e EU residency — Claude Enterprise, Azure OpenAI</span></li>
          <li><span class="ans-label">NO</span><span class="ans-text">Soli modelli self-hosted — Llama 3, Mistral, Qwen, Gemma su infrastruttura propria</span></li>
        </ul>
      </div>
      <div class="dec-card">
        <div class="dc-num">Domanda 04</div>
        <div class="dc-question">Quale context window serve?</div>
        <ul class="dc-answers">
          <li><span class="ans-label">&lt;32K</span><span class="ans-text">Conversazioni standard, documenti brevi → tutti i modelli la coprono</span></li>
          <li><span class="ans-label">32K–200K</span><span class="ans-text">Documenti lunghi, codebase interi → Claude (200K), GPT-4o (128K), Gemini (128K)</span></li>
          <li><span class="ans-label">&gt;200K</span><span class="ans-text">Interi repository, libri, dataset → Claude 3 Opus/Sonnet (200K), Gemini 1.5 Pro (1M)</span></li>
        </ul>
      </div>
      <div class="dec-card">
        <div class="dc-num">Domanda 05</div>
        <div class="dc-question">Qual è il volume atteso e il budget?</div>
        <ul class="dc-answers">
          <li><span class="ans-label">BASSO</span><span class="ans-text">&lt;10K richieste/mese → costo marginale, usa il modello migliore per qualità</span></li>
          <li><span class="ans-label">MEDIO</span><span class="ans-text">10K–1M richieste/mese → ottimizza il mix: frontier per task critici, fast per task semplici</span></li>
          <li><span class="ans-label">ALTO</span><span class="ans-text">&gt;1M richieste/mese → caching, batch API, modelli più economici o self-hosted obbligatori</span></li>
        </ul>
      </div>
      <div class="dec-card">
        <div class="dc-num">Regola pratica</div>
        <div class="dc-question">Partenza sempre dal modello migliore</div>
        <ul class="dc-answers">
          <li><span class="ans-label">PERCHÉ</span><span class="ans-text">Stabilisci il baseline di qualità massima raggiungibile sul tuo task specifico</span></li>
          <li><span class="ans-label">POI</span><span class="ans-text">Scendi verso modelli più leggeri finché la qualità rimane accettabile sull'eval set</span></li>
          <li><span class="ans-label">MAI</span><span class="ans-text">Partire dal modello più economico sperando funzioni — spesso non funziona e perdi tempo</span></li>
        </ul>
      </div>
    </div>
  </div>

  <!-- 1.2 Panoramica modelli -->
  <div class="section">
    <div class="sec-label">Capitolo I · Sezione 2</div>
    <div class="sec-h2">Panoramica dei modelli<br>principali oggi</div>
    <p class="sec-intro">
      Il panorama cambia velocemente — i modelli qui rappresentano lo stato a febbraio 2026. I prezzi indicati sono indicativi. <strong>Verifica sempre i prezzi aggiornati</strong> sulle pricing page ufficiali prima di stimare i costi di produzione.
    </p>

    <div class="model-grid">

      <div class="model-card tier-frontier">
        <div class="mc-tier">Tier Frontier · Massima qualità</div>
        <div class="mc-name">Claude Opus 4</div>
        <div class="mc-family">Anthropic · claude-opus-4-6</div>
        <div class="mc-desc">
          Il modello più capace di Anthropic. <strong>Ragionamento esteso, coding complesso, analisi sfumata.</strong> Context window 200K. Ottimo per task dove la qualità dell'output vale il costo e la latenza.
        </div>
        <div class="mc-tags">
          <span class="mc-tag best">Coding avanzato</span>
          <span class="mc-tag best">Analisi complessa</span>
          <span class="mc-tag best">Ragionamento multi-step</span>
          <span class="mc-tag avoid">Chat real-time</span>
          <span class="mc-tag avoid">Alto volume</span>
        </div>
        <div class="mc-stats">
          <div class="mc-stat"><span class="mc-stat-n" style="color:#c0a050">200K</span><span class="mc-stat-l">Context</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#c0a050">~$15</span><span class="mc-stat-l">/1M token in</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#c0a050">Alta</span><span class="mc-stat-l">Latenza</span></div>
        </div>
      </div>

      <div class="model-card tier-frontier">
        <div class="mc-tier">Tier Frontier · Massima qualità</div>
        <div class="mc-name">GPT-4o</div>
        <div class="mc-family">OpenAI · gpt-4o</div>
        <div class="mc-desc">
          Il flagship di OpenAI. <strong>Forte su multimodale (immagini, audio), coding, ragionamento.</strong> Context 128K. Ecosistema maturo con molte integrazioni. Leggermente più veloce di Opus su task standard.
        </div>
        <div class="mc-tags">
          <span class="mc-tag best">Multimodale</span>
          <span class="mc-tag best">Coding</span>
          <span class="mc-tag good">Ragionamento</span>
          <span class="mc-tag avoid">Privacy stretta (DPA necessario)</span>
        </div>
        <div class="mc-stats">
          <div class="mc-stat"><span class="mc-stat-n" style="color:#c0a050">128K</span><span class="mc-stat-l">Context</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#c0a050">~$5</span><span class="mc-stat-l">/1M token in</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#c0a050">Media</span><span class="mc-stat-l">Latenza</span></div>
        </div>
      </div>

      <div class="model-card tier-mid">
        <div class="mc-tier">Tier Bilanciato · Qualità + Velocità</div>
        <div class="mc-name">Claude Sonnet 4.5</div>
        <div class="mc-family">Anthropic · claude-sonnet-4-5</div>
        <div class="mc-desc">
          Il punto dolce di Anthropic. <strong>Qualità molto vicina a Opus a un terzo del costo e con latenza significativamente inferiore.</strong> La scelta ottimale per la maggior parte dei task aziendali. Ottimo per agenti con RAG, coding assistito, analisi documentale.
        </div>
        <div class="mc-tags">
          <span class="mc-tag best">RAG / Documentale</span>
          <span class="mc-tag best">Agenti aziendali</span>
          <span class="mc-tag best">Coding assistito</span>
          <span class="mc-tag good">Chat interattiva</span>
        </div>
        <div class="mc-stats">
          <div class="mc-stat"><span class="mc-stat-n" style="color:#4acab8">200K</span><span class="mc-stat-l">Context</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#4acab8">~$3</span><span class="mc-stat-l">/1M token in</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#4acab8">Media</span><span class="mc-stat-l">Latenza</span></div>
        </div>
      </div>

      <div class="model-card tier-mid">
        <div class="mc-tier">Tier Bilanciato · Qualità + Velocità</div>
        <div class="mc-name">Gemini 1.5 Pro</div>
        <div class="mc-family">Google · gemini-1.5-pro</div>
        <div class="mc-desc">
          Il modello più competitivo su <strong>context window lunghe — fino a 1M token</strong>. Forte su documenti molto lunghi, video, audio. Ottima scelta quando devi processare interi repository o dataset non troncabili.
        </div>
        <div class="mc-tags">
          <span class="mc-tag best">Context ultra-lungo (1M)</span>
          <span class="mc-tag best">Video/Audio</span>
          <span class="mc-tag good">Documenti voluminosi</span>
          <span class="mc-tag avoid">Task short context</span>
        </div>
        <div class="mc-stats">
          <div class="mc-stat"><span class="mc-stat-n" style="color:#4acab8">1M</span><span class="mc-stat-l">Context</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#4acab8">~$3.5</span><span class="mc-stat-l">/1M token in</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#4acab8">Media</span><span class="mc-stat-l">Latenza</span></div>
        </div>
      </div>

      <div class="model-card tier-fast">
        <div class="mc-tier">Tier Fast · Velocità + Economia</div>
        <div class="mc-name">Claude Haiku 3.5</div>
        <div class="mc-family">Anthropic · claude-haiku-3-5</div>
        <div class="mc-desc">
          Il modello più veloce ed economico di Anthropic. <strong>Latenza sub-secondo, costo ~20x inferiore a Opus.</strong> Sorprendentemente capace su task strutturati: classificazione, estrazione dati, routing, riassunti brevi.
        </div>
        <div class="mc-tags">
          <span class="mc-tag best">Classificazione</span>
          <span class="mc-tag best">Estrazione dati</span>
          <span class="mc-tag best">Router agente</span>
          <span class="mc-tag good">Riassunti brevi</span>
          <span class="mc-tag avoid">Ragionamento complesso</span>
        </div>
        <div class="mc-stats">
          <div class="mc-stat"><span class="mc-stat-n" style="color:#9aaae0">200K</span><span class="mc-stat-l">Context</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#9aaae0">~$0.8</span><span class="mc-stat-l">/1M token in</span></div>
          <div class="mc-stat"><span class="mc-stat-n" style="color:#9aaae0">Bassa</span><span class="mc-stat-l">Latenza</span></div>
        </div>
      </div>

      <div class="model-card tier-local">
        <div class="mc-tier">Tier Self-hosted · Privacy assoluta</div>
        <div class="mc-name">Llama 3.3 70B / Mistral Large</div>
        <div class="mc-family">Meta / Mistral · via Ollama, vLLM, Together AI</div>
        <div class="mc-desc">
          Modelli open-weight deployabili su infrastruttura propria. <strong>Dati che non escono mai dall'organizzazione.</strong> Qualità molto buona per task strutturati, ancora inferiore ai frontier su ragionamento complesso. Richiedono GPU (A100/H100) per performance accettabili a scala.
        </div>
        <div class="mc-tags">
          <span class="mc-tag best">Privacy assoluta</span>
          <span class="mc-tag best">Zero costo per token</span>
          <span class="mc-tag good">Task strutturati</span>
          <span class="mc-tag avoid">Ragionamento frontier-level</span>
          <span class="mc-tag avoid">Senza GPU adeguate</span>
        </div>
        <div class="mc-stats">
          <div class="mc-stat"><span class="mc-stat-n">128K</span><span class="mc-stat-l">Context</span></div>
          <div class="mc-stat"><span class="mc-stat-n">€0</span><span class="mc-stat-l">Per token</span></div>
          <div class="mc-stat"><span class="mc-stat-n">GPU</span><span class="mc-stat-l">Dipende HW</span></div>
        </div>
      </div>

    </div>
  </div>

  <!-- 1.3 Matrice task/modello -->
  <div class="section">
    <div class="sec-label">Capitolo I · Sezione 3</div>
    <div class="sec-h2">Matrice task → modello consigliato</div>
    <p class="sec-intro">
      Per ogni categoria di task, quale modello scegliere come primo tentativo. <strong>Ricorda: queste sono raccomandazioni di partenza</strong>, non verità assolute — il tuo eval set sul task specifico decide.
    </p>

    <table class="task-matrix">
      <thead>
        <tr>
          <th>Task</th>
          <th>Prima scelta</th>
          <th>Alternativa</th>
          <th>Evita</th>
          <th>Motivazione chiave</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><span class="task-name">RAG su documenti aziendali</span></td>
          <td class="cell-best">Claude Sonnet 4.5</td>
          <td class="cell-good">GPT-4o</td>
          <td class="cell-avoid">Haiku (per doc complessi)</td>
          <td>Long context 200K, ottima instruction following, fedeltà al documento</td>
        </tr>
        <tr>
          <td><span class="task-name">Coding e code review</span></td>
          <td class="cell-best">Claude Sonnet 4.5 / Opus 4</td>
          <td class="cell-good">GPT-4o</td>
          <td class="cell-avoid">Modelli &lt;7B self-hosted</td>
          <td>Claude eccelle su ragionamento codice, spiegazioni, bug detection</td>
        </tr>
        <tr>
          <td><span class="task-name">Classificazione email/ticket</span></td>
          <td class="cell-best">Claude Haiku 3.5</td>
          <td class="cell-good">GPT-4o mini</td>
          <td class="cell-avoid">Opus (overkill e costoso)</td>
          <td>Task strutturato semplice → velocità e costo battono qualità frontier</td>
        </tr>
        <tr>
          <td><span class="task-name">Estrazione dati strutturati</span></td>
          <td class="cell-best">Claude Haiku 3.5</td>
          <td class="cell-good">Mistral 7B self-hosted</td>
          <td class="cell-avoid">Modelli senza JSON mode</td>
          <td>Usa output strutturato (JSON mode). Schema rigoroso compensa modello leggero</td>
        </tr>
        <tr>
          <td><span class="task-name">Customer service chat</span></td>
          <td class="cell-best">Claude Sonnet 4.5</td>
          <td class="cell-good">GPT-4o mini</td>
          <td class="cell-note">Haiku se FAQ semplici</td>
          <td>Bilanciamento qualità/latenza. Haiku accettabile solo per domini molto ristretti</td>
        </tr>
        <tr>
          <td><span class="task-name">Analisi finanziaria / legale</span></td>
          <td class="cell-best">Claude Opus 4</td>
          <td class="cell-good">GPT-4o</td>
          <td class="cell-avoid">Qualsiasi modello &lt;70B</td>
          <td>Ragionamento preciso e sfumato obbligatorio. Errori costosi → investi nella qualità</td>
        </tr>
        <tr>
          <td><span class="task-name">Generazione contenuti marketing</span></td>
          <td class="cell-best">Claude Sonnet 4.5</td>
          <td class="cell-good">GPT-4o</td>
          <td class="cell-note">Dipende dal brand voice</td>
          <td>Testa entrambi con il tuo brand guide come system prompt — le preferenze variano</td>
        </tr>
        <tr>
          <td><span class="task-name">Router agente (decide quale tool)</span></td>
          <td class="cell-best">Claude Haiku 3.5</td>
          <td class="cell-good">GPT-4o mini</td>
          <td class="cell-avoid">Frontier (troppo costoso per routing)</td>
          <td>Il router fa una classificazione semplice → latenza minima, costo minimo</td>
        </tr>
        <tr>
          <td><span class="task-name">Documenti lunghissimi (&gt;200K token)</span></td>
          <td class="cell-best">Gemini 1.5 Pro</td>
          <td class="cell-good">Claude Opus 4 (con chunking)</td>
          <td class="cell-avoid">Modelli con context &lt;128K</td>
          <td>Gemini 1.5 Pro è l'unico con 1M context nativo senza chunking</td>
        </tr>
        <tr>
          <td><span class="task-name">Dati sensibili / privacy assoluta</span></td>
          <td class="cell-best">Llama 3.3 70B (self-hosted)</td>
          <td class="cell-good">Mistral Large (self-hosted)</td>
          <td class="cell-avoid">Qualsiasi modello cloud senza DPA</td>
          <td>Self-hosting obbligatorio se i dati non possono uscire dall'infrastruttura</td>
        </tr>
      </tbody>
    </table>
  </div>

</div>


<!-- ═══════════════════════════
     CAPITOLO II — VALUTAZIONE
═══════════════════════════ -->
<div class="chapter" id="ch2">

  <div class="section">
    <div class="sec-label">Capitolo II · Sezione 1</div>
    <div class="sec-h2">Come valutare le performance:<br>il metodo corretto</div>
    <p class="sec-intro">
      I benchmark pubblici (MMLU, HumanEval, MATH) misurano capacità generali in condizioni di laboratorio. <strong>Non predicono le performance sul tuo task specifico.</strong> L'unico modo per sapere quale modello funziona meglio per te è costruire un eval set tuo e misurare direttamente.
    </p>

    <div class="alert warn">
      <div class="alert-icon">⚠</div>
      <div class="alert-body"><strong>Il problema dei leaderboard pubblici:</strong> i modelli vengono ottimizzati sui benchmark più famosi (MMLU, MATH, HumanEval). Un modello che occupa la prima posizione in classifica può essere significativamente peggiore di un altro sul tuo caso d'uso specifico. MMLU misura conoscenza generale; tu hai bisogno di sapere chi è meglio nel classificare i tuoi ticket di supporto.</div>
    </div>

    <div class="eval-phases">
      <div class="ep-num" style="grid-row: 1">1</div>
      <div class="ep-content">
        <div class="ep-title">Costruire il tuo eval set</div>
        <div class="ep-desc">Raccogli <strong>50–200 esempi reali</strong> del tuo task: input reali che riceverà l'agente + output corretto atteso. Più è grande e vario l'eval set, più i risultati sono affidabili. Meno di 30 esempi dà risultati statisticamente non significativi. Gli esempi devono coprire i casi tipici E i casi limite (input anomali, edge case, richieste ambigue).</div>
      </div>

      <div class="ep-num" style="grid-row: 2">2</div>
      <div class="ep-content">
        <div class="ep-title">Definire la metrica giusta per il task</div>
        <div class="ep-desc">Non esiste una metrica universale. <strong>Scegli in base a cosa conta davvero:</strong> per classificazione usa accuracy e F1. Per estrazione dati usa precision/recall sui campi estratti. Per generazione testo usa LLM-as-judge con rubrica specifica (0–5). Per task agente usa task success rate end-to-end. Per RAG usa answer faithfulness (la risposta è supportata dai documenti?) + answer relevance.</div>
      </div>

      <div class="ep-num" style="grid-row: 3">3</div>
      <div class="ep-content">
        <div class="ep-title">Eseguire in modo controllato</div>
        <div class="ep-desc">Esegui ogni modello candidato su <strong>tutti gli stessi esempi</strong>, con lo stesso system prompt, gli stessi parametri (temperatura, max_tokens). Cambia una sola variabile alla volta. Se cambi sia il modello che il prompt non sai quale dei due ha prodotto il miglioramento. Registra tutto: modello, versione, timestamp, parametri usati.</div>
      </div>

      <div class="ep-num" style="grid-row: 4">4</div>
      <div class="ep-content">
        <div class="ep-title">Analizzare gli errori, non solo la media</div>
        <div class="ep-desc">Un modello con accuracy 85% potrebbe essere perfetto o inaccettabile a seconda di quali sono il 15% di errori. <strong>Analizza manualmente ogni errore:</strong> il modello ha frainteso la domanda? Ha allucinato? Ha prodotto formato sbagliato? Ha sbagliato su un tipo specifico di input? Questa analisi qualitativa è più preziosa del numero aggregato.</div>
      </div>

      <div class="ep-num" style="grid-row: 5">5</div>
      <div class="ep-content">
        <div class="ep-title">Misurare costo e latenza reali</div>
        <div class="ep-desc">Durante l'eval, registra anche: <strong>token usati per input/output</strong> (per calcolare il costo reale per task), latenza media e p95 (il 95° percentile — i casi lenti), frequenza di errori API (rate limiting, timeout). Questi numeri ti danno il Total Cost of Ownership reale, non quello stimato dalla pricing page.</div>
      </div>
    </div>
  </div>

  <div class="section">
    <div class="sec-label">Capitolo II · Sezione 2</div>
    <div class="sec-h2">Benchmark pubblici:<br>cosa misurano davvero</div>
    <p class="sec-intro">
      Conosci i benchmark per interpretarli correttamente — non per scegliere il modello in base a loro.
    </p>

    <table class="bench-table">
      <thead>
        <tr>
          <th>Benchmark</th>
          <th>Cosa misura</th>
          <th>Utile per</th>
          <th>Limiti critici</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><span class="bname">MMLU</span></td>
          <td>Conoscenza generale su 57 domini (matematica, storia, legge, medicina...)</td>
          <td>Capire la "larghezza" della knowledge base del modello</td>
          <td><span class="bwarn">Non predice performance su task specifici aziendali. Modelli spesso ottimizzati su questo dataset.</span></td>
        </tr>
        <tr>
          <td><span class="bname">HumanEval / SWE-bench</span></td>
          <td>Capacità di scrivere codice funzionante su problemi standard / fix bug in repo reali</td>
          <td>Scegliere il modello per un coding agent o code review</td>
          <td><span class="bwarn">HumanEval misura algoritmi classici, non necessariamente il tipo di codice che scrivi tu.</span></td>
        </tr>
        <tr>
          <td><span class="bname">MATH / AIME</span></td>
          <td>Ragionamento matematico e problem solving quantitativo</td>
          <td>Task che richiedono ragionamento step-by-step preciso</td>
          <td><span class="bwarn">Poco rilevante per la maggior parte dei task business. Proxy molto indiretto per "ragiona bene".</span></td>
        </tr>
        <tr>
          <td><span class="bname">LMSYS Chatbot Arena</span></td>
          <td>Preferenze umane in conversazioni comparative (Elo rating)</td>
          <td>Capire quale modello produce output preferiti dagli umani in generale</td>
          <td><span class="bwarn">Le preferenze sono aggregate su molti task diversi. La tua preferenza su un task specifico può divergere significativamente.</span></td>
        </tr>
        <tr>
          <td><span class="bname">RAGAS / TruLens</span></td>
          <td>Qualità del retrieval RAG: faithfulness, relevance, context recall</td>
          <td>Valutare e confrontare pipeline RAG — direttamente utile</td>
          <td><span class="bwarn">Misura il framework RAG più che il modello. Utile ma richiede integrazione nel tuo pipeline.</span></td>
        </tr>
        <tr>
          <td><span class="bname">Il tuo eval set custom</span></td>
          <td>Performance sul tuo task specifico, con i tuoi dati reali</td>
          <td>Scegliere il modello per il tuo caso d'uso — l'unico benchmark che conta</td>
          <td style="color:#4acab8">Richiede investimento iniziale per costruirlo, ma è l'unico dato affidabile per le tue decisioni.</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="section">
    <div class="sec-label">Capitolo II · Sezione 3</div>
    <div class="sec-h2">LLM-as-judge:<br>valutazione automatica scalabile</div>
    <p class="sec-intro">
      La human evaluation è costosa e lenta. Per scalare la valutazione, puoi usare un secondo LLM come "giudice" che valuta l'output del primo. Funziona bene per qualità soggettiva — non per correttezza fattuale.
    </p>

    <div class="alert info">
      <div class="alert-icon">ℹ</div>
      <div class="alert-body"><strong>Quando funziona bene:</strong> valutare tono, coerenza con istruzioni, completezza della risposta, appropriatezza del formato. <strong>Quando non funziona:</strong> verificare correttezza di calcoli, accuratezza di dati fattuali, correttezza del codice. Per questi casi, usa valutazione programmatica o human review.</div>
    </div>

    <div class="code-wrap">
      <div class="code-label">Prompt LLM-as-judge — template pratico</div>
      <div class="code-body">
<span class="cm"># Sistema: il giudice valuta l'output dell'agente</span><br>
<span class="hl">JUDGE_SYSTEM_PROMPT</span> = <span class="st">"""</span><br>
<span class="st">Sei un valutatore esperto. Il tuo compito è valutare</span><br>
<span class="st">la qualità di una risposta generata da un assistente AI.</span><br>
<br>
<span class="st">Valuta la risposta secondo questi criteri (1-5 ognuno):</span><br>
<span class="st">1. CORRETTEZZA: La risposta è accurata rispetto al contesto fornito?</span><br>
<span class="st">2. COMPLETEZZA: Risponde a tutti gli aspetti della domanda?</span><br>
<span class="st">3. FEDELTÀ: Si basa solo sul contesto, senza inventare?</span><br>
<span class="st">4. TONO: È appropriata per un contesto professionale?</span><br>
<br>
<span class="st">Rispondi SOLO con JSON: {"correttezza": N, "completezza": N,</span><br>
<span class="st">"fedeltà": N, "tono": N, "score_totale": N, "note": "..."}</span><br>
<span class="st">"""</span><br>
<br>
<span class="kw">def</span> <span class="fn">judge_response</span>(<span class="pl">question, context, response, judge_model</span>=<span class="st">"claude-sonnet-4-5"</span>):<br>
&nbsp;&nbsp;<span class="pl">prompt</span> = <span class="st">f"""</span><br>
<span class="st">&nbsp;&nbsp;DOMANDA UTENTE: {question}</span><br>
<span class="st">&nbsp;&nbsp;CONTESTO FORNITO: {context}</span><br>
<span class="st">&nbsp;&nbsp;RISPOSTA DA VALUTARE: {response}</span><br>
<span class="st">&nbsp;&nbsp;"""</span><br>
&nbsp;&nbsp;<span class="cm"># Chiama il modello giudice e parsa il JSON</span><br>
&nbsp;&nbsp;<span class="pl">result</span> = <span class="fn">call_llm</span>(<span class="pl">judge_model, JUDGE_SYSTEM_PROMPT, prompt</span>)<br>
&nbsp;&nbsp;<span class="kw">return</span> <span class="fn">json.loads</span>(<span class="pl">result</span>)
      </div>
    </div>
  </div>

</div>


<!-- ═══════════════════════════
     CAPITOLO III — CONFIGURAZIONE
═══════════════════════════ -->
<div class="chapter" id="ch3">

  <div class="section">
    <div class="sec-label">Capitolo III · Sezione 1</div>
    <div class="sec-h2">Parametri di configurazione:<br>cosa controlla cosa</div>
    <p class="sec-intro">
      I parametri modificano il comportamento del modello in modo prevedibile. <strong>La maggior parte dei problemi di comportamento si risolve con prompt engineering</strong>, non con i parametri — ma i parametri hanno effetti specifici che è fondamentale capire.
    </p>

    <div class="param-table">
      <div class="pt-header">
        <span>Parametri principali — comportamento e raccomandazioni</span>
        <span>Verde = valore consigliato come punto di partenza</span>
      </div>
      <div class="pt-row head">
        <div class="ptc">Parametro</div>
        <div class="ptc">Range</div>
        <div class="ptc">Effetto e quando modificarlo</div>
        <div class="ptc">Valori per task</div>
      </div>

      <div class="pt-row">
        <div class="ptc">
          <span class="param-name">temperature</span>
          <span class="param-range">0.0 → 2.0</span>
        </div>
        <div class="ptc"><strong>Controllo casualità.</strong> 0 = deterministico (sempre la stessa risposta), 1 = bilanciato, >1 = molto creativo/imprevedibile. <em>Non è una manopola di "intelligenza"</em> — cambia solo quanto il modello esplora alternative meno probabili.</div>
        <div class="ptc">
          <div class="val-chip rec">0.0 → estrazione/classificazione</div>
          <div class="val-chip rec">0.2 → RAG, QA tecnica</div>
          <div class="val-chip rec">0.5 → risposta conversazionale</div>
          <div class="val-chip">0.7–1.0 → copywriting, brainstorm</div>
          <div class="val-chip">>1.0 → poetry, creatività libera</div>
        </div>
      </div>

      <div class="pt-row">
        <div class="ptc">
          <span class="param-name">max_tokens</span>
          <span class="param-range">1 → model limit</span>
        </div>
        <div class="ptc"><strong>Lunghezza massima output.</strong> Non influenza la qualità, solo tronca la risposta se raggiunge il limite. Impostarlo troppo basso tronca risposte a metà. Impostarlo al massimo aumenta i costi se il modello produce output verbose. Calibra sul 95° percentile della lunghezza attesa.</div>
        <div class="ptc">
          <div class="val-chip rec">256 → classificazione/estrazione</div>
          <div class="val-chip rec">1024 → risposta standard</div>
          <div class="val-chip rec">4096 → analisi, report</div>
          <div class="val-chip">8192+ → codice lungo, documenti</div>
        </div>
      </div>

      <div class="pt-row">
        <div class="ptc">
          <span class="param-name">top_p</span>
          <span class="param-range">0.0 → 1.0</span>
        </div>
        <div class="ptc"><strong>Nucleus sampling.</strong> Considera solo i token che coprono la probabilità cumulativa top_p. Alternativa a temperature per controllare la casualità. <strong>Regola pratica: usa temperature OR top_p, non entrambi.</strong> La maggior parte dei task funziona bene con il default (1.0). Abbassarlo (0.9) riduce output improbabili senza eliminare variabilità.</div>
        <div class="ptc">
          <div class="val-chip rec">1.0 → default, quasi sempre OK</div>
          <div class="val-chip">0.9 → ridurre output insoliti</div>
          <div class="val-chip">0.5 → output molto conservativo</div>
        </div>
      </div>

      <div class="pt-row">
        <div class="ptc">
          <span class="param-name">stop_sequences</span>
          <span class="param-range">Lista di stringhe</span>
        </div>
        <div class="ptc"><strong>Il modello si ferma quando genera una di queste stringhe.</strong> Utile per output strutturati: ferma la generazione dopo il JSON chiuso, dopo un tag XML, dopo una riga specifica. Riduce i token sprecati e rende l'output più prevedibile da parsare.</div>
        <div class="ptc">
          <div class="val-chip rec">["```"] → dopo code block</div>
          <div class="val-chip rec">["</response>"] → tag XML</div>
          <div class="val-chip">["FINE", "---"] → separatori custom</div>
        </div>
      </div>

      <div class="pt-row">
        <div class="ptc">
          <span class="param-name">system</span>
          <span class="param-range">Testo libero</span>
        </div>
        <div class="ptc"><strong>Il parametro più impattante di tutti.</strong> Il system prompt definisce il comportamento dell'agente, il suo ruolo, i vincoli, il formato dell'output. Una modifica al system prompt ha effetti molto più grandi di qualsiasi modifica ai parametri numerici. La sezione successiva è dedicata interamente a questo.</div>
        <div class="ptc">
          <div class="val-chip rec">Sempre specificato</div>
          <div class="val-chip rec">Strutturato con sezioni</div>
          <div class="val-chip rec">Testato sistematicamente</div>
        </div>
      </div>

    </div>
  </div>

  <div class="section">
    <div class="sec-label">Capitolo III · Sezione 2</div>
    <div class="sec-h2">Anatomia del system prompt<br>efficace</div>
    <p class="sec-intro">
      Il system prompt non è una nota a piè di pagina — è il documento fondativo del comportamento dell'agente. <strong>Un system prompt mal scritto produce un agente inaffidabile indipendentemente dal modello scelto.</strong>
    </p>

    <div class="prompt-anatomy">
      <div class="pa-header">Struttura raccomandata — System Prompt per agente aziendale</div>

      <div class="pa-segment">
        <div class="pa-label-cell">
          <div class="pa-seg-name">1. Ruolo e identità</div>
          <div class="pa-seg-opt req">OBBLIGATORIO</div>
        </div>
        <div class="pa-content-cell">
          <div class="pa-seg-desc">Chi è l'agente, per quale organizzazione lavora, qual è la sua specializzazione. <strong>Sii specifico: non "assistente utile" ma "analista di supporto clienti per [Azienda]".</strong> Il modello si comporta in modo coerente con il ruolo dichiarato.</div>
          <div class="pa-example">Sei l'assistente di supporto tecnico di Acme S.r.l., specializzato nell'assistenza ai clienti del software ERP AcmeSuite versioni 3.x e 4.x.</div>
        </div>
      </div>

      <div class="pa-segment">
        <div class="pa-label-cell">
          <div class="pa-seg-name">2. Obiettivo primario</div>
          <div class="pa-seg-opt req">OBBLIGATORIO</div>
        </div>
        <div class="pa-content-cell">
          <div class="pa-seg-desc">Una frase che definisce cosa deve fare il modello. <strong>Usa verbi di azione precisi: "classifica", "estrai", "redigi", "rispondi"</strong> — non "aiuta" o "supporta" che sono troppo vaghi.</div>
          <div class="pa-example">Il tuo obiettivo è rispondere alle domande degli utenti sul software AcmeSuite usando esclusivamente la documentazione fornita nel contesto.</div>
        </div>
      </div>

      <div class="pa-segment">
        <div class="pa-label-cell">
          <div class="pa-seg-name">3. Vincoli espliciti</div>
          <div class="pa-seg-opt req">OBBLIGATORIO</div>
        </div>
        <div class="pa-content-cell">
          <div class="pa-seg-desc">Cosa l'agente NON deve fare. I vincoli negativi sono spesso più importanti di quelli positivi — definiscono i confini dell'autonomia. <strong>Sii specifico, non generico</strong> ("non inventare numeri di telefono" è meglio di "non inventare informazioni").</div>
          <div class="pa-example">NON fornire mai: prezzi, sconti, o impegni commerciali. NON rispondere a domande su software concorrenti. Se non trovi la risposta nella documentazione, dì esplicitamente "Non ho questa informazione nella mia base di conoscenza".</div>
        </div>
      </div>

      <div class="pa-segment">
        <div class="pa-label-cell">
          <div class="pa-seg-name">4. Formato output</div>
          <div class="pa-seg-opt req">OBBLIGATORIO</div>
        </div>
        <div class="pa-content-cell">
          <div class="pa-seg-desc">Come deve essere strutturata la risposta. Lunghezza attesa, uso di liste o prosa, lingua, tono. <strong>Se l'output alimenta un sistema, specifica il formato esatto (JSON, XML) con lo schema.</strong></div>
          <div class="pa-example">Rispondi in italiano, in modo professionale ma accessibile. Lunghezza: 2-4 paragrafi per domande standard. Usa elenchi puntati solo per step sequenziali. Per errori tecnici, includi sempre: causa probabile + soluzione + link documentazione.</div>
        </div>
      </div>

      <div class="pa-segment">
        <div class="pa-label-cell">
          <div class="pa-seg-name">5. Gestione incertezza</div>
          <div class="pa-seg-opt req">OBBLIGATORIO</div>
        </div>
        <div class="pa-content-cell">
          <div class="pa-seg-desc">Cosa fare quando il modello non sa la risposta. <strong>Senza questa istruzione, il modello tende ad allucinare piuttosto che ammettere incertezza.</strong> Specifica esattamente il comportamento atteso.</div>
          <div class="pa-example">Se non sei sicuro della risposta o non trovi informazioni nella documentazione: (1) dillo esplicitamente, (2) non speculare, (3) suggerisci di contattare il supporto tecnico all'indirizzo support@acme.it.</div>
        </div>
      </div>

      <div class="pa-segment">
        <div class="pa-label-cell">
          <div class="pa-seg-name">6. Esempi (few-shot)</div>
          <div class="pa-seg-opt">CONSIGLIATO</div>
        </div>
        <div class="pa-content-cell">
          <div class="pa-seg-desc">2–5 coppie domanda/risposta ideale che mostrano concretamente il comportamento atteso. <strong>Gli esempi sono più efficaci delle istruzioni astratte</strong> per definire tono, lunghezza, formato. Aggiornali quando scopri pattern di errore ricorrenti.</div>
          <div class="pa-example">ESEMPIO: Utente: "Come esporto il bilancio in PDF?" → Assistente: "Per esportare il bilancio in PDF in AcmeSuite 4.x: 1. Vai in Contabilità → Reportistica 2. Seleziona il periodo..."</div>
        </div>
      </div>

    </div>

    <div class="alert info">
      <div class="alert-icon">💡</div>
      <div class="alert-body"><strong>Differenza tra modelli sul system prompt:</strong> Claude segue particolarmente bene istruzioni strutturate con tag XML (<code>&lt;role&gt;</code>, <code>&lt;constraints&gt;</code>, <code>&lt;format&gt;</code>) — usa questa struttura per istruzioni complesse. GPT-4o risponde bene a stile markdown con intestazioni. I modelli self-hosted (Llama, Mistral) richiedono spesso istruzioni più semplici e dirette.</div>
    </div>
  </div>

  <div class="section">
    <div class="sec-label">Capitolo III · Sezione 3</div>
    <div class="sec-h2">Best practice di prompt engineering<br>per agenti</div>

    <div class="rules-list">
      <div class="rule-row">
        <div class="rr-num">1</div>
        <div class="rr-content">
          <div class="rr-title">Una modifica alla volta, poi misura</div>
          <div class="rr-desc">Non cambiare più elementi del prompt contemporaneamente. Se cambi il tono E aggiungi un esempio E modifichi il formato, non sai cosa ha prodotto il miglioramento. <strong>Tratta il prompt come un esperimento scientifico: una variabile alla volta.</strong></div>
        </div>
      </div>
      <div class="rule-row">
        <div class="rr-num">2</div>
        <div class="rr-content">
          <div class="rr-title">Usa XML per strutturare istruzioni complesse (con Claude)</div>
          <div class="rr-desc">
            <strong>Con Claude specificamente</strong>, usare tag XML migliora sistematicamente la capacità del modello di seguire istruzioni complesse. Il modello è stato addestrato a rispettare questa struttura.
          </div>
        </div>
      </div>
      <div class="rule-row">
        <div class="rr-num">3</div>
        <div class="rr-content">
          <div class="rr-title">Chiedi il ragionamento prima della risposta (chain-of-thought)</div>
          <div class="rr-desc">Per task di ragionamento complesso, aggiungere "Ragiona passo per passo prima di rispondere" migliora significativamente la qualità. Il modello produce meno errori quando "pensa ad alta voce" nel contesto. <strong>Attenzione: aumenta i token output e quindi i costi.</strong></div>
        </div>
      </div>
      <div class="rule-row">
        <div class="rr-num">4</div>
        <div class="rr-content">
          <div class="rr-title">Specifica il formato dell'output con uno schema esatto</div>
          <div class="rr-desc">Se l'output deve essere JSON, includi lo schema nel prompt. Non scrivere "rispondi in JSON" — scrivi "rispondi esattamente in questo formato JSON: {'campo1': 'stringa', 'campo2': numero}". L'output strutturato + schema riduce quasi a zero gli errori di parsing.</div>
        </div>
      </div>
      <div class="rule-row">
        <div class="rr-num">5</div>
        <div class="rr-content">
          <div class="rr-title">Aggiungi esempi negativi per i casi limite</div>
          <div class="rr-desc">Oltre agli esempi di comportamento corretto, includi esempi di <strong>cosa NON fare</strong>: "Se l'utente chiede prezzi, NON rispondere con un numero — invece di: 'Il costo è €500' scrivi: 'Per informazioni sui prezzi, contatta commerciale@acme.it'". I negative examples sono più efficaci delle istruzioni astratte sui casi limite.</div>
        </div>
      </div>
      <div class="rule-row">
        <div class="rr-num">6</div>
        <div class="rr-content">
          <div class="rr-title">Versiona il prompt come il codice</div>
          <div class="rr-desc">Ogni versione del system prompt deve essere in version control (Git o piattaforma come LangSmith). Devi poter rispondere a "come funzionava l'agente il mese scorso?" e tornare a quella versione in minuti se una modifica introduce una regressione.</div>
        </div>
      </div>
    </div>

    <div class="code-wrap">
      <div class="code-label">Esempio: System prompt strutturato con XML per Claude</div>
      <div class="code-body">
<span class="hl">&lt;role&gt;</span><br>
<span class="pl">Sei l'assistente di analisi contratti di Acme Legal S.r.l.</span><br>
<span class="pl">Hai expertise in contrattualistica italiana e diritto commerciale.</span><br>
<span class="hl">&lt;/role&gt;</span><br>
<br>
<span class="hl">&lt;objective&gt;</span><br>
<span class="pl">Analizza contratti forniti dall'utente e identifica:</span><br>
<span class="pl">1. Clausole a rischio per il cliente</span><br>
<span class="pl">2. Obbligazioni principali di entrambe le parti</span><br>
<span class="pl">3. Termini e condizioni di rinnovo/rescissione</span><br>
<span class="hl">&lt;/objective&gt;</span><br>
<br>
<span class="hl">&lt;constraints&gt;</span><br>
<span class="pl">- Basa OGNI affermazione sul testo contrattuale fornito</span><br>
<span class="pl">- Cita SEMPRE l'articolo/paragrafo di riferimento</span><br>
<span class="pl">- NON fornire pareri legali vincolanti</span><br>
<span class="pl">- Se un termine è ambiguo, segnalalo esplicitamente</span><br>
<span class="hl">&lt;/constraints&gt;</span><br>
<br>
<span class="hl">&lt;output_format&gt;</span><br>
<span class="pl">{"rischi": [{"clausola": "art.X", "descrizione": "...", "gravità": "alta|media|bassa"}],</span><br>
<span class="pl"> "obbligazioni": {"cliente": [...], "fornitore": [...]},</span><br>
<span class="pl"> "note_critiche": "..."}</span><br>
<span class="hl">&lt;/output_format&gt;</span>
      </div>
    </div>
  </div>

</div>


<!-- ═══════════════════════════
     CAPITOLO IV — COSTI
═══════════════════════════ -->
<div class="chapter" id="ch4">

  <div class="section">
    <div class="sec-label">Capitolo IV · Sezione 1</div>
    <div class="sec-h2">Struttura dei costi:<br>come si calcola la spesa reale</div>
    <p class="sec-intro">
      I modelli cloud si pagano per token — unità di testo (~0.75 parole). Il costo di un'operazione dipende da <strong>token in ingresso (prompt) + token in uscita (risposta)</strong>. Gli output costano tipicamente 3–5× di più degli input. Il costo per task non è solo il costo della risposta — è la somma di ogni chiamata nel loop dell'agente.
    </p>

    <div class="cost-tiers">
      <div class="cost-tier">
        <div class="ct-tier">Tier Frontier</div>
        <div class="ct-cost" style="color:#c0a050">$10–75</div>
        <div class="ct-unit">per milione token output</div>
        <div class="ct-models"><strong>Claude Opus 4</strong>, GPT-4o, Gemini 1.5 Ultra. Per task critici dove la qualità vale il costo. Evita per task ad alto volume.</div>
      </div>
      <div class="cost-tier">
        <div class="ct-tier">Tier Bilanciato</div>
        <div class="ct-cost" style="color:#4acab8">$3–15</div>
        <div class="ct-unit">per milione token output</div>
        <div class="ct-models"><strong>Claude Sonnet 4.5</strong>, GPT-4o-mini avanzato, Gemini 1.5 Pro. Il punto dolce per la maggior parte dei task aziendali.</div>
      </div>
      <div class="cost-tier">
        <div class="ct-tier">Tier Fast</div>
        <div class="ct-cost" style="color:#9aaae0">$0.5–4</div>
        <div class="ct-unit">per milione token output</div>
        <div class="ct-models"><strong>Claude Haiku 3.5</strong>, GPT-4o mini, Gemini Flash. Per task strutturati, routing, classificazione ad alto volume.</div>
      </div>
      <div class="cost-tier">
        <div class="ct-tier">Self-hosted</div>
        <div class="ct-cost">€0</div>
        <div class="ct-unit">per token (solo infrastruttura)</div>
        <div class="ct-models"><strong>Llama 3.3, Mistral, Qwen</strong> su GPU proprie. Costo fisso infrastruttura, zero per token. Economico a scala.</div>
      </div>
    </div>

    <div class="code-wrap">
      <div class="code-label">Calcolare il costo reale per task — agente con loop</div>
      <div class="code-body">
<span class="cm"># Un agente con 3 iterazioni su Claude Sonnet 4.5</span><br>
<span class="cm"># Prezzi indicativi (verifica sempre su anthropic.com/pricing)</span><br>
<br>
<span class="pl">PRICE_IN</span>  = <span class="nu">3.00</span>   <span class="cm"># $/1M token input (Sonnet 4.5)</span><br>
<span class="pl">PRICE_OUT</span> = <span class="nu">15.00</span>  <span class="cm"># $/1M token output (Sonnet 4.5)</span><br>
<br>
<span class="cm"># Per task: system prompt + 3 iterazioni agente</span><br>
<span class="pl">system_prompt_tokens</span>  = <span class="nu">800</span>    <span class="cm"># token in input ogni chiamata</span><br>
<span class="pl">avg_context_per_iter</span>  = <span class="nu">1200</span>   <span class="cm"># contesto cresce a ogni iter</span><br>
<span class="pl">avg_output_per_iter</span>   = <span class="nu">400</span>    <span class="cm"># risposta/ragionamento del modello</span><br>
<span class="pl">num_iterations</span>        = <span class="nu">3</span><br>
<br>
<span class="pl">total_input</span>  = (system_prompt_tokens + avg_context_per_iter) * num_iterations</span><br>
<span class="pl">total_output</span> = avg_output_per_iter * num_iterations</span><br>
<br>
<span class="pl">cost_per_task</span> = (total_input/1_000_000 * PRICE_IN) + (total_output/1_000_000 * PRICE_OUT)<br>
<span class="cm"># ≈ $0.012 per task — 1.2 centesimi</span><br>
<span class="cm"># A 10.000 task/mese → ~$120/mese</span><br>
<span class="cm"># A 100.000 task/mese → ~$1.200/mese  ← qui inizia l'ottimizzazione</span>
      </div>
    </div>
  </div>

  <div class="section">
    <div class="sec-label">Capitolo IV · Sezione 2</div>
    <div class="sec-h2">7 strategie di ottimizzazione<br>dei costi senza perdere qualità</div>
    <p class="sec-intro">
      Queste strategie si applicano in sequenza: inizia dalla prima (più semplice) e applica le successive solo quando il volume giustifica la complessità aggiuntiva.
    </p>

    <div class="opt-grid">

      <div class="opt-card">
        <div class="oc-label">Strategia 01 · Immediatamente applicabile</div>
        <div class="oc-title">Prompt Caching</div>
        <div class="oc-desc">
          Anthropic e OpenAI offrono caching automatico del prefix del prompt. Se il system prompt (uguale su tutte le chiamate) viene cachato, <strong>paghi solo la prima chiamata per intero — le successive pagano solo i nuovi token</strong>. Attivato automaticamente su Claude per prompt >1024 token.
        </div>
        <div class="oc-saving">Risparmio tipico: 60–90% sui costi input</div>
      </div>

      <div class="opt-card">
        <div class="oc-label">Strategia 02 · Immediatamente applicabile</div>
        <div class="oc-title">Batch API per task non real-time</div>
        <div class="oc-desc">
          Claude e OpenAI offrono una Batch API per elaborazioni asincrone: invii un batch di richieste, vengono elaborate entro 24 ore, <strong>a circa il 50% del prezzo standard</strong>. Ideale per analisi documenti notturne, generazione report periodici, processing dati non urgente.
        </div>
        <div class="oc-saving">Risparmio: 50% rispetto al prezzo standard</div>
      </div>

      <div class="opt-card">
        <div class="oc-label">Strategia 03 · Applicabile con poco sforzo</div>
        <div class="oc-title">Model routing: modelli diversi per task diversi</div>
        <div class="oc-desc">
          Non usare il modello frontier per tutto. <strong>Usa un modello leggero (Haiku) come router</strong> che classifica la richiesta, poi instrada a Sonnet per task medi e a Opus solo per task genuinamente complessi. La maggior parte delle richieste reali sono semplici.
        </div>
        <div class="oc-saving">Risparmio tipico: 40–70% su agenti ad alto volume</div>
      </div>

      <div class="opt-card">
        <div class="oc-label">Strategia 04 · Applicabile con poco sforzo</div>
        <div class="oc-title">Ottimizzare la lunghezza del sistema prompt</div>
        <div class="oc-desc">
          Ogni token del system prompt viene inviato ad ogni chiamata. Un system prompt di 2000 token su 100K richieste/mese = 200M token aggiuntivi. <strong>Elimina le ridondanze, accorcia gli esempi, rimuovi le istruzioni per casi che non si presentano mai.</strong>
        </div>
        <div class="oc-saving">Risparmio: lineare con la riduzione del prompt</div>
      </div>

      <div class="opt-card">
        <div class="oc-label">Strategia 05 · Richiede analisi</div>
        <div class="oc-title">Caching applicativo delle risposte frequenti</div>
        <div class="oc-desc">
          Se alcune domande si ripetono spesso, cacha la risposta a livello applicativo (Redis, database). Prima di chiamare il modello, controlla se la stessa domanda (o molto simile) è già stata risposta recentemente. <strong>Funziona bene per FAQ, cataloghi prodotti, policy aziendali.</strong>
        </div>
        <div class="oc-saving">Risparmio: dipende da frequenza ripetizioni</div>
      </div>

      <div class="opt-card">
        <div class="oc-label">Strategia 06 · Richiede setup tecnico</div>
        <div class="oc-title">Compressione del contesto (context distillation)</div>
        <div class="oc-desc">
          In agenti con loop lunghi, il contesto cresce a ogni iterazione. Invece di passare tutta la storia, usa un <strong>modello leggero per riassumere periodicamente il contesto</strong> — il riassunto entra nel prompt successivo invece della trascrizione completa. Riduce i token di input significativamente.
        </div>
        <div class="oc-saving">Risparmio: 30–60% su agenti con loop lunghi</div>
      </div>

      <div class="opt-card">
        <div class="oc-label">Strategia 07 · Solo a volumi molto alti</div>
        <div class="oc-title">Fine-tuning o modello self-hosted</div>
        <div class="oc-desc">
          A volume molto alto (>1M richieste/mese), può convenire fare <strong>fine-tuning di un modello open su dati propri</strong>, o deployare Llama/Mistral self-hosted. Break-even tipico: se la GPU costa €3K/mese e il cloud costava €10K/mese, il self-hosting conviene. Calcola sempre il TCO completo.
        </div>
        <div class="oc-saving">Risparmio: potenzialmente 70–90% ma con costi fissi</div>
      </div>

      <div class="opt-card">
        <div class="oc-label">Regola generale</div>
        <div class="oc-title">Misura prima, ottimizza dopo</div>
        <div class="oc-desc">
          <strong>Non ottimizzare i costi prima di avere dati reali di produzione.</strong> Le stime pre-deploy sono spesso sbagliate di un ordine di grandezza — in entrambe le direzioni. Monitora il costo per task fin dal giorno 1 e intervieni solo quando i numeri reali lo giustificano.
        </div>
        <div class="oc-saving">Priorità: sempre prima la correttezza, poi i costi</div>
      </div>

    </div>
  </div>

  <div class="section">
    <div class="sec-label">Capitolo IV · Sezione 3</div>
    <div class="sec-h2">Monitoring dei costi in produzione</div>

    <div class="code-wrap">
      <div class="code-label">Tracking costi per task — implementazione minima</div>
      <div class="code-body">
<span class="kw">import</span> <span class="pl">anthropic, time, structlog</span><br>
<span class="pl">log</span> = <span class="fn">structlog.get_logger</span>()<br>
<br>
<span class="cm"># Prezzi Sonnet 4.5 (aggiorna da anthropic.com/pricing)</span><br>
<span class="pl">COST_PER_M_IN</span>  = <span class="nu">3.00</span>   <span class="cm"># $ per milione token input</span><br>
<span class="pl">COST_PER_M_OUT</span> = <span class="nu">15.00</span>  <span class="cm"># $ per milione token output</span><br>
<br>
<span class="kw">def</span> <span class="fn">call_with_tracking</span>(<span class="pl">messages, system, task_id</span>):<br>
&nbsp;&nbsp;<span class="pl">start</span> = <span class="fn">time.time</span>()<br>
&nbsp;&nbsp;<span class="pl">response</span> = <span class="pl">client.messages.create(</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl">model</span>=<span class="st">"claude-sonnet-4-5"</span>,<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl">system</span>=<span class="pl">system, messages</span>=<span class="pl">messages, max_tokens</span>=<span class="nu">2048</span><br>
&nbsp;&nbsp;<span class="pl">)</span><br>
<br>
&nbsp;&nbsp;<span class="pl">tokens_in</span>  = <span class="pl">response.usage.input_tokens</span><br>
&nbsp;&nbsp;<span class="pl">tokens_out</span> = <span class="pl">response.usage.output_tokens</span><br>
&nbsp;&nbsp;<span class="pl">cost_usd</span> = (tokens_in/1e6 * COST_PER_M_IN) + (tokens_out/1e6 * COST_PER_M_OUT)</span><br>
<br>
&nbsp;&nbsp;<span class="fn">log.info</span>(<span class="st">"llm_call"</span>,<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl">task_id</span>=<span class="pl">task_id</span>,<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl">tokens_in</span>=<span class="pl">tokens_in, tokens_out</span>=<span class="pl">tokens_out</span>,<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl">cost_usd</span>=<span class="fn">round</span>(<span class="pl">cost_usd, 6</span>),<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="pl">latency_ms</span>=<span class="fn">int</span>((<span class="fn">time.time</span>()-<span class="pl">start)</span>*<span class="nu">1000</span>)<br>
&nbsp;&nbsp;<span class="pl">)</span><br>
&nbsp;&nbsp;<span class="kw">return</span> <span class="pl">response</span>
      </div>
    </div>

    <div class="alert danger">
      <div class="alert-icon">⚑</div>
      <div class="alert-body"><strong>Alert obbligatorio sul budget:</strong> imposta sempre un hard cap di spesa giornaliera/mensile sull'account API (Anthropic e OpenAI lo permettono dalla console). Un agente con un bug nel loop o sotto attacco può generare migliaia di dollari di costi in poche ore senza questo controllo. Non è un'opzione — è un prerequisito del deploy.</div>
    </div>

  </div>

</div>


<!-- FOOTER -->
<footer>
  <span>Agenti AI · Selezione e configurazione modelli LLM · Febbraio 2026</span>
  <span>agenti-ai-llm-selection.html</span>
</footer>

<script>
function showChapter(id) {
  document.querySelectorAll('.chapter').forEach(c => c.classList.remove('active'));
  document.querySelectorAll('.cn-tab').forEach(t => t.classList.remove('active'));
  document.getElementById(id).classList.add('active');
  const idx = ['ch1','ch2','ch3','ch4'].indexOf(id);
  document.querySelectorAll('.cn-tab')[idx].classList.add('active');
  window.scrollTo({ top: document.querySelector('.chapter-nav').offsetTop - 10, behavior: 'smooth' });
}
</script>
</body>
</html>
