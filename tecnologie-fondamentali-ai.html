<!DOCTYPE html>
<html lang="it">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tecnologie Fondamentali per Famiglia AI</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&family=IBM+Plex+Sans:wght@300;400;500&family=IBM+Plex+Mono:wght@400;500&display=swap');

:root {
  --bg: #f9f7f2;
  --ink: #1e1c18;
  --muted: #7a7568;
  --rule: #dedad2;
  --surface: #f0ede5;

  --c1: #1d4e7a;   /* ML Classico — blu */
  --c2: #5a3278;   /* Deep Learning — viola */
  --c3: #1a6644;   /* LLM — verde */
}

* { box-sizing: border-box; margin: 0; padding: 0; }

body {
  background: var(--bg);
  color: var(--ink);
  font-family: 'IBM Plex Sans', sans-serif;
  font-size: 15px;
  line-height: 1.75;
  max-width: 860px;
  margin: 0 auto;
  padding: 60px 48px 100px;
}

@media(max-width:620px){ body { padding: 36px 20px 80px; } }

/* ── HEADER ── */
header {
  border-bottom: 2px solid var(--ink);
  padding-bottom: 36px;
  margin-bottom: 52px;
}
.kicker {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 10px;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 18px;
}
h1 {
  font-family: 'Libre Baskerville', serif;
  font-size: clamp(26px, 4.5vw, 42px);
  font-weight: 400;
  line-height: 1.15;
  letter-spacing: -0.015em;
  margin-bottom: 14px;
}
h1 em { font-style: italic; }
.subtitle {
  color: var(--muted);
  font-size: 15px;
  font-weight: 300;
  max-width: 560px;
}

/* ── FAMILY BLOCK ── */
.family {
  margin-bottom: 56px;
  padding-bottom: 56px;
  border-bottom: 1px solid var(--rule);
}
.family:last-child { border-bottom: none; margin-bottom: 0; }

.family-header {
  display: flex;
  align-items: flex-start;
  gap: 20px;
  margin-bottom: 28px;
}
.family-index {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 34px;
  font-weight: 400;
  line-height: 1;
  opacity: 0.18;
  flex-shrink: 0;
  padding-top: 4px;
}
.family-title-group {}
.family-label {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 9px;
  letter-spacing: 0.18em;
  text-transform: uppercase;
  margin-bottom: 4px;
}
.family-name {
  font-family: 'Libre Baskerville', serif;
  font-size: clamp(20px, 3vw, 28px);
  font-weight: 400;
  line-height: 1.15;
}
.family-tagline {
  font-size: 13px;
  color: var(--muted);
  margin-top: 4px;
  font-weight: 300;
}

/* ── TECH CARD ── */
.tech {
  background: white;
  border: 1.5px solid var(--rule);
  border-radius: 5px;
  padding: 24px 28px;
  margin-bottom: 14px;
  position: relative;
  overflow: hidden;
}
.tech::before {
  content: '';
  position: absolute;
  top: 0; left: 0;
  width: 4px;
  height: 100%;
  border-radius: 5px 0 0 5px;
}
.tech-header {
  display: flex;
  align-items: baseline;
  gap: 12px;
  margin-bottom: 8px;
  flex-wrap: wrap;
}
.tech-name {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 13.5px;
  font-weight: 500;
  color: var(--ink);
}
.tech-role {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 9px;
  letter-spacing: 0.12em;
  text-transform: uppercase;
  padding: 2px 8px;
  border-radius: 2px;
  color: white;
  font-weight: 400;
}
.tech-desc {
  font-size: 14px;
  font-weight: 300;
  line-height: 1.7;
  color: var(--ink);
  margin-bottom: 16px;
}

/* Three-column info row */
.tech-cols {
  display: grid;
  grid-template-columns: 1fr 1fr 1fr;
  gap: 1px;
  background: var(--rule);
  border: 1px solid var(--rule);
  border-radius: 3px;
  overflow: hidden;
  margin-top: 14px;
}
@media(max-width:580px){ .tech-cols { grid-template-columns: 1fr; } }
.tech-col {
  background: var(--surface);
  padding: 12px 14px;
}
.tech-col-label {
  font-family: 'IBM Plex Mono', monospace;
  font-size: 8.5px;
  letter-spacing: 0.12em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 5px;
}
.tech-col-text {
  font-size: 13px;
  line-height: 1.55;
  color: var(--ink);
  font-weight: 300;
}
.tech-col-text strong { font-weight: 500; }

/* Improvement tag */
.improvement {
  display: inline-flex;
  align-items: center;
  gap: 8px;
  margin-top: 14px;
  padding: 7px 14px;
  border-radius: 3px;
  font-size: 12.5px;
  font-weight: 400;
  line-height: 1.45;
}
.improvement::before {
  content: '↑';
  font-size: 14px;
  flex-shrink: 0;
}

/* Colors */
.c1 { color: var(--c1); }
.c2 { color: var(--c2); }
.c3 { color: var(--c3); }
.bg1 { background: var(--c1); }
.bg2 { background: var(--c2); }
.bg3 { background: var(--c3); }
.bar1::before { background: var(--c1); }
.bar2::before { background: var(--c2); }
.bar3::before { background: var(--c3); }
.imp1 { background: rgba(29,78,122,0.07); color: var(--c1); }
.imp2 { background: rgba(90,50,120,0.07); color: var(--c2); }
.imp3 { background: rgba(26,102,68,0.07); color: var(--c3); }

/* ── SUMMARY TABLE ── */
.summary {
  margin-top: 60px;
  padding-top: 48px;
  border-top: 2px solid var(--ink);
}
.summary-title {
  font-family: 'Libre Baskerville', serif;
  font-size: 20px;
  font-weight: 400;
  margin-bottom: 24px;
}
table {
  width: 100%;
  border-collapse: collapse;
  font-size: 13px;
}
th {
  background: var(--ink);
  color: var(--bg);
  padding: 9px 14px;
  text-align: left;
  font-family: 'IBM Plex Mono', monospace;
  font-size: 9px;
  letter-spacing: 0.12em;
  text-transform: uppercase;
  font-weight: 400;
}
td {
  padding: 11px 14px;
  border-bottom: 1px solid var(--rule);
  vertical-align: top;
  line-height: 1.5;
  font-weight: 300;
}
tr:nth-child(even) td { background: var(--surface); }
td:first-child { font-family: 'IBM Plex Mono', monospace; font-size: 12px; font-weight: 500; }
td:nth-child(2) { font-style: italic; color: var(--muted); }
</style>
</head>
<body>

<header>
  <div class="kicker">Report sintetico · Intelligenza Artificiale</div>
  <h1>Tecnologie fondamentali<br>per <em>famiglia AI</em></h1>
  <p class="subtitle">Per ciascuna delle tre principali famiglie di IA: la tecnologia o le tecnologie abilitanti, come funzionano, e quale salto qualitativo hanno reso possibile.</p>
</header>


<!-- ══════════════════════════════════════ -->
<!-- FAMIGLIA 1 — ML CLASSICO -->
<!-- ══════════════════════════════════════ -->
<div class="family">
  <div class="family-header">
    <div class="family-index c1">01</div>
    <div class="family-title-group">
      <div class="family-label c1">Prima famiglia</div>
      <div class="family-name">ML Classico</div>
      <div class="family-tagline">Apprendimento supervisionato su feature costruite a mano</div>
    </div>
  </div>

  <!-- TECH 1.1 -->
  <div class="tech bar1">
    <div class="tech-header">
      <span class="tech-name">Modelli statistico-matematici</span>
      <span class="tech-role bg1">Tecnologia primaria</span>
    </div>
    <p class="tech-desc">Il ML classico si fonda sulla statistica matematica: ogni algoritmo (regressione lineare/logistica, SVM, alberi decisionali, Random Forest, gradient boosting) è una funzione matematica che apprende parametri ottimali minimizzando un <strong>errore misurabile</strong> su dati storici etichettati. Il processo: un ingegnere umano seleziona le variabili rilevanti (<em>feature engineering</em>), il modello apprende i pesi statisticamente ottimali per quelle variabili, produce previsioni come output.</p>
    <div class="tech-cols">
      <div class="tech-col">
        <div class="tech-col-label">Come funziona</div>
        <div class="tech-col-text">Dati storici → funzione di perdita → ottimizzazione dei parametri (es. discesa del gradiente) → modello con pesi calibrati su misura.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Cosa ha permesso</div>
        <div class="tech-col-text">Prima sistematizzazione dell'apprendimento automatico: un computer che <strong>impara regole dai dati</strong> invece di seguire solo regole scritte a mano.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Limite superato</div>
        <div class="tech-col-text">Rispetto ai sistemi simbolici: nessuna necessità di codificare manualmente ogni regola. Il modello le inferisce statisticamente dall'esperienza.</div>
      </div>
    </div>
    <div class="improvement imp1">Passaggio da regole manuali a regole apprese dai dati — nasce l'apprendimento automatico misurabile e riproducibile.</div>
  </div>

  <!-- TECH 1.2 -->
  <div class="tech bar1">
    <div class="tech-header">
      <span class="tech-name">Funzioni di perdita e ottimizzazione convessa</span>
      <span class="tech-role bg1">Tecnologia secondaria</span>
    </div>
    <p class="tech-desc">Il meccanismo che rende i modelli statistici <em>addestrabili</em> è la <strong>funzione di perdita</strong>: una formula che quantifica quanto le previsioni del modello si discostino dalla realtà. Tecniche come la discesa del gradiente trovano i parametri che la minimizzano. L'intuizione chiave: se la funzione di perdita è <em>convessa</em> (ha un unico minimo globale), l'ottimizzazione è garantita convergere alla soluzione migliore possibile — una proprietà matematicamente dimostrabile che rende il training affidabile e prevedibile.</p>
    <div class="tech-cols">
      <div class="tech-col">
        <div class="tech-col-label">Come funziona</div>
        <div class="tech-col-text">Calcola l'errore → misura come i parametri influenzano l'errore (gradiente) → aggiusta i parametri nella direzione che riduce l'errore → ripete fino a convergenza.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Cosa ha permesso</div>
        <div class="tech-col-text">Training <strong>stabile, teoricamente garantito e interpretabile</strong>: è possibile dimostrare matematicamente che il modello troverà la soluzione ottimale.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Limite superato</div>
        <div class="tech-col-text">Rispetto all'euristica manuale: la qualità del modello non dipende dall'intuizione del programmatore ma da un processo di ottimizzazione matematicamente fondato.</div>
      </div>
    </div>
    <div class="improvement imp1">Training matematicamente garantito: sapere che il modello converge alla soluzione ottimale trasforma il ML in disciplina ingegneristica affidabile.</div>
  </div>
</div>


<!-- ══════════════════════════════════════ -->
<!-- FAMIGLIA 2 — DEEP LEARNING -->
<!-- ══════════════════════════════════════ -->
<div class="family">
  <div class="family-header">
    <div class="family-index c2">02</div>
    <div class="family-title-group">
      <div class="family-label c2">Seconda famiglia</div>
      <div class="family-name">Deep Learning</div>
      <div class="family-tagline">Reti neurali profonde che apprendono le proprie feature</div>
    </div>
  </div>

  <!-- TECH 2.1 -->
  <div class="tech bar2">
    <div class="tech-header">
      <span class="tech-name">Reti neurali profonde (Deep Neural Networks)</span>
      <span class="tech-role bg2">Tecnologia primaria</span>
    </div>
    <p class="tech-desc">Una rete neurale profonda è una struttura a strati (<em>layer</em>) di unità computazionali ispirate ai neuroni biologici. Ogni strato riceve l'output del precedente, applica una trasformazione non-lineare, e passa il risultato allo strato successivo. La parola <strong>"profonda"</strong> si riferisce al numero di strati: decine, centinaia, a volte migliaia. Ogni strato impara a riconoscere pattern di complessità crescente: i primi strati rilevano strutture semplici (bordi in un'immagine, suoni elementari), quelli profondi assemblano quelle strutture in concetti astratti (volti, parole, oggetti). Nessuno programa esplicitamente queste rappresentazioni intermedie: <strong>emergono dall'addestramento</strong>.</p>
    <div class="tech-cols">
      <div class="tech-col">
        <div class="tech-col-label">Come funziona</div>
        <div class="tech-col-text">Input grezzo → strati che trasformano la rappresentazione progressivamente → output finale. I pesi di ogni connessione vengono aggiustati via backpropagation per ridurre l'errore.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Cosa ha permesso</div>
        <div class="tech-col-text">Eliminare il <strong>feature engineering manuale</strong>: la rete costruisce automaticamente le rappresentazioni interne più utili per il compito. Immagini, audio, testo elaborati direttamente.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Limite superato</div>
        <div class="tech-col-text">Rispetto al ML classico: non serve un esperto di dominio che decida quali variabili usare. Il modello scopre da solo le feature rilevanti nei dati grezzi.</div>
      </div>
    </div>
    <div class="improvement imp2">Feature engineering automatico: il modello apprende cosa guardare nei dati, non solo come pesare variabili già selezionate da un umano.</div>
  </div>

  <!-- TECH 2.2 -->
  <div class="tech bar2">
    <div class="tech-header">
      <span class="tech-name">Backpropagation + GPU parallelism</span>
      <span class="tech-role bg2">Tecnologia secondaria</span>
    </div>
    <p class="tech-desc">Le reti neurali esistevano teoricamente dagli anni '60, ma erano inutilizzabili in pratica per due ragioni: non si sapeva come addestrare reti con molti strati, e i calcoli richiesti erano troppo lenti. La <strong>backpropagation</strong> (1986, Rumelhart et al.) ha risolto il primo problema: un algoritmo che propaga l'errore all'indietro attraverso tutti gli strati, calcolando esattamente quanto ogni peso contribuisce all'errore e di quanto aggiustarlo. L'hardware GPU (originariamente progettato per la grafica) ha risolto il secondo: migliaia di operazioni in parallelo invece di migliaia in sequenza, rendendo l'addestramento da anni a ore.</p>
    <div class="tech-cols">
      <div class="tech-col">
        <div class="tech-col-label">Come funziona</div>
        <div class="tech-col-text">Forward pass → calcolo errore → propagazione del gradiente all'indietro strato per strato (regola della catena del calcolo) → aggiornamento dei pesi. Su GPU: tutto in parallelo.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Cosa ha permesso</div>
        <div class="tech-col-text">Addestrare reti con <strong>miliardi di parametri</strong> in tempi ragionevoli. Senza GPU, AlexNet (2012, la rete che ha rilanciato il deep learning) avrebbe richiesto anni invece di settimane.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Limite superato</div>
        <div class="tech-col-text">Rispetto al ML classico: rende praticabile l'addestramento di modelli con rappresentazioni interne gerarchiche a qualsiasi profondità.</div>
      </div>
    </div>
    <div class="improvement imp2">Scalabilità pratica: backpropagation + GPU ha trasformato le reti neurali da curiosità teorica a tecnologia industriale — il salto che ha generato la rivoluzione del deep learning 2012-2017.</div>
  </div>
</div>


<!-- ══════════════════════════════════════ -->
<!-- FAMIGLIA 3 — LLM -->
<!-- ══════════════════════════════════════ -->
<div class="family">
  <div class="family-header">
    <div class="family-index c3">03</div>
    <div class="family-title-group">
      <div class="family-label c3">Terza famiglia</div>
      <div class="family-name">LLM (Large Language Models)</div>
      <div class="family-tagline">Architettura Transformer addestrata su scala massiva</div>
    </div>
  </div>

  <!-- TECH 3.1 -->
  <div class="tech bar3">
    <div class="tech-header">
      <span class="tech-name">Architettura Transformer + Self-Attention</span>
      <span class="tech-role bg3">Tecnologia primaria</span>
    </div>
    <p class="tech-desc">Il Transformer (Google, 2017 — "Attention Is All You Need") è l'architettura che ha reso possibili gli LLM. Il suo contributo centrale è il meccanismo di <strong>self-attention</strong>: ogni parola in una sequenza calcola quanto ciascuna delle altre parole è rilevante per interpretare se stessa, producendo una rappresentazione contestuale dinamica (la stessa parola "banca" viene rappresentata diversamente in "banca dati" e "banca del fiume"). Crucialmente, questo calcolo avviene <strong>in parallelo</strong> su tutta la sequenza — non in sequenza come nelle architetture precedenti (RNN). Questo risolve contemporaneamente il problema delle dipendenze a lunga distanza e quello della parallelizzabilità del training.</p>
    <div class="tech-cols">
      <div class="tech-col">
        <div class="tech-col-label">Come funziona</div>
        <div class="tech-col-text">Ogni token genera tre vettori (Query, Key, Value). Il prodotto Q·K misura la rilevanza reciproca; la softmax normalizza i pesi; la somma pesata dei Value produce la rappresentazione contestuale finale.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Cosa ha permesso</div>
        <div class="tech-col-text"><strong>Training parallelo su scala enorme</strong>: addestrare modelli con centinaia di miliardi di parametri su trilioni di token — computazionalmente impossibile con architetture sequenziali.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Limite superato</div>
        <div class="tech-col-text">Rispetto alle RNN: nessuna degradazione della memoria a lunga distanza; nessuna sequenzialità che blocca il parallelismo; scala in modo predicibile all'aumentare delle risorse.</div>
      </div>
    </div>
    <div class="improvement imp3">Parallelismo + contesto globale: ogni elemento può influenzare ogni altro indipendentemente dalla distanza — il prerequisito architetturale per addestrare modelli dell'ordine dei miliardi di parametri.</div>
  </div>

  <!-- TECH 3.2 -->
  <div class="tech bar3">
    <div class="tech-header">
      <span class="tech-name">Pre-training su scala + RLHF</span>
      <span class="tech-role bg3">Tecnologia secondaria</span>
    </div>
    <p class="tech-desc">Il Transformer è la struttura; il <strong>pre-training su larga scala</strong> è ciò che la riempie di conoscenza. Addestrare il modello su trilioni di token con l'obiettivo "predici il token successivo" costringe la rete a costruire internamente una rappresentazione del mondo — grammatica, fatti, logica, stili — come effetto collaterale. Il risultato è un <em>foundation model</em>: potente ma grezzo. Il <strong>RLHF</strong> (Reinforcement Learning from Human Feedback) è la fase che lo trasforma in assistente utile: valutatori umani confrontano risposte diverse, addestrando un modello separato a predire la preferenza umana, che viene poi usato per affinare il comportamento dell'LLM principale. È il passaggio che ha reso questi sistemi praticamente utilizzabili.</p>
    <div class="tech-cols">
      <div class="tech-col">
        <div class="tech-col-label">Come funziona</div>
        <div class="tech-col-text">Pre-training: miliardi di esempi "predici il prossimo token" → conoscenza emerge. RLHF: confronti A/B umani → reward model → ottimizzazione dell'LLM per massimizzare il reward.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Cosa ha permesso</div>
        <div class="tech-col-text">Costruire <strong>conoscenza generalista senza supervision esplicita</strong>, poi affinarla con feedback umano per produrre risposte utili, sicure e allineate alle aspettative.</div>
      </div>
      <div class="tech-col">
        <div class="tech-col-label">Limite superato</div>
        <div class="tech-col-text">Rispetto al fine-tuning classico: non serve etichettare manualmente milioni di esempi di risposta corretta — basta comparare quale risposta è preferibile, molto più scalabile umanamente.</div>
      </div>
    </div>
    <div class="improvement imp3">Da modello grezzo a assistente allineato: il pre-training genera la conoscenza, l'RLHF la rende utilizzabile — insieme hanno prodotto il salto percettivo tra GPT-2 e ChatGPT.</div>
  </div>
</div>


<!-- ══════════════════════════════════════ -->
<!-- TABELLA RIEPILOGATIVA -->
<!-- ══════════════════════════════════════ -->
<div class="summary">
  <div class="summary-title">Riepilogo comparativo</div>
  <table>
    <thead>
      <tr>
        <th>Famiglia</th>
        <th>Metafora del meccanismo</th>
        <th>Tecnologia primaria</th>
        <th>Tecnologia secondaria</th>
        <th>Salto abilitato</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="color:var(--c1)">ML Classico</td>
        <td>Ottimizzazione di equazioni su misure scelte da un esperto</td>
        <td>Modelli statistico-matematici (regressione, SVM, gradient boosting…)</td>
        <td>Funzioni di perdita + ottimizzazione convessa</td>
        <td>Dalle regole manuali a regole apprese — primo vero apprendimento automatico</td>
      </tr>
      <tr>
        <td style="color:var(--c2)">Deep Learning</td>
        <td>Gerarchia di filtri che costruiscono concetti sempre più astratti</td>
        <td>Reti neurali profonde (feature learning automatico)</td>
        <td>Backpropagation + GPU parallelism</td>
        <td>Dal feature engineering manuale a rappresentazioni apprese — elaborazione di dati grezzi (immagini, audio)</td>
      </tr>
      <tr>
        <td style="color:var(--c3)">LLM</td>
        <td>Ogni parola "guarda" tutte le altre e aggiorna il proprio significato in base al contesto</td>
        <td>Transformer + Self-Attention (parallelismo + contesto globale)</td>
        <td>Pre-training su larga scala + RLHF</td>
        <td>Dal fine-tuning supervisato a conoscenza generalista emergente — comprensione e generazione del linguaggio naturale</td>
      </tr>
    </tbody>
  </table>
</div>

</body>
</html>
